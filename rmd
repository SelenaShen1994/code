---
title: "occ preparation"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r library, message=FALSE}
library(tidyverse)
library(Hmisc)
library(lubridate)
library(Rmisc)
library(rpart)
library(rpart.plot)
library(caret)
library("GGally")
library(Metrics)

```

### tidy data

```{r tidy data, eval=FALSE}
library(readr)
SPY.Historical.Data<-read_csv("SPY Historical Data.csv")
head(SPY.Historical.Data,3)
library(dplyr)
SPY.Historical.Data <-SPY.Historical.Data%>%janitor::clean_names() #clean names
glimpse(SPY.Historical.Data ) #take a look at the data
```

### Summary data

```{r summary,eval=FALSE}
#correlation table
wrangleData %>% 
  select(price_change_lag_1, price_change_lag_2, price_change_lag_3, change) %>% #keep Y variable last
  ggcorr(method = c("pairwise", "pearson"), label_round=2, label = TRUE) 

#encode numeric to categorical
combined<-mutate(wrangleData,direction=as.factor(ifelse(change>0.6,1,0)))#define direction variable

#factor transformation 
levels(combined$direction) <- make.names(levels(factor(combined$direction)))

data_clean <- data %>% select(-"lang",-"html",-"suppressed",-"lower_page") %>%
  mutate(month = month(created_at),
         year = year(created_at),
         yr_mth = paste0(year,"-",month)) %>% 
  filter(yr_mth >"2018-04") %>% filter(yr_mth<"2019-06")

ad_analysis<-ad_analysis %>% filter(str_detect(message, "Trump")) #15119 rows
ad_analysis %>% group_by(advertiser) %>% summarise(count = n()) %>% arrange(desc(count)) %>% head(5) %>% 
  ggplot(aes(x = reorder(advertiser,count),y=count,fill = advertiser))+geom_col()

```

### plot

```{r plot,eval=FALSE}

library(ggplot2)
#plot line
ggplot(SPY.Historical.Data, aes(x=date,y=price))+ geom_line()

#plothistogram
ggplot(SPY.Historical.Data, aes(x=change)) + 
  geom_histogram(fill="black", position="dodge",binwidth=0.5) #histogram of daily changes

#plot histogram by group
ggplot(SPY.Historical.Data, aes(x=change)) + 
  geom_histogram(fill="black", position="dodge",binwidth=0.5)+scale_color_brewer(palette="Paired")+facet_grid(day_of_week ~ .)

#plot scattered plot
ggplot(SPY.Historical.Data, aes(change, day_of_week)) +
  geom_jitter(aes(color = day_of_week), size = 0)+
  ggpubr::color_palette("jco")+
  ggpubr::theme_pubclean()

#plot point
ggplot(wrangleData, aes(y=change,x=price_change_lag_1))+ geom_point() #change vs change the day before

#bar plot
discrepancy %>% count(yr_mth) %>%   
         mutate(yr_mth = factor(yr_mth, levels = date_level)) %>% 
  ggplot(aes(yr_mth, n, fill=yr_mth)) +
  geom_bar(stat="identity")
```

### train test split

```{r train test split,eval=FALSE}
library(rsample)
set.seed(100) 
train_test_split <- initial_split(combined, prop = 0.75) #training set contains 75% of the data
train <- training(train_test_split)
test <- testing(train_test_split)
combined_train<-train
combined_test<-test

```


### feature selection

```{r,eval=FALSE}
control <- trainControl (
    method="CV",
    number=5,
    verboseIter=FALSE)

#Find the best model with 1, 2, up to 20 variables with backward induction
BackFit <- train(int_rate ~ poly(loan_amnt,3) + term+ dti + annual_inc + grade +grade:poly(loan_amnt,3):term +poly(loan_amnt,3):term +grade:term ,
    lc_clean,
    method = "leapBackward", #can chance method to "leapSeq", "leapForward"
    tuneGrid = data.frame(nvmax = 1:20), #Will find the best model with 1:20 variables. 
    trControl = control
)

#show the results of all models
BackFit$results
#simmarize the model of best fit and its coefficients
#summary(BackFit$finalModel) #depending on the number of models estimated, the output of this command could be long
coef(BackFit$finalModel,BackFit$bestTune$nvmax)

```



### linear regression

```{r linear regression,eval=FALSE}

control <- trainControl (
    method="cv",
    number=10,
    verboseIter=TRUE) #by setting this to true the model will report its progress after each estimation

#we are going to train the model and report the results using k-fold cross validation
plsFit<-train(
    int_rate ~ loan_amnt + term+ dti + annual_inc + grade ,
    lc_clean,
   method = "lm",
    trControl = control
   )
  

summary(plsFit)
```

### logistic regression

```{r logistic regression,eval=FALSE}
logreg<-glm(formula=direction ~.-date-change, 
                    data = combined_train,family = "binomial") #Fit a logistic regression model
summary(logreg)

logreg$xlevels[["date"]] <- union(logreg$xlevels[["date"]], levels(combined_test$date)) #make sure all the levels are recognized

predictions<- logreg%>% predict(combined_test, type = "response") #predict the probabilities
summary(predictions)

library(pROC)
ROC_lr_improve <- roc(combined_test$direction, predictions) #find roc
AUC_lr_improve<-round(ROC_lr_improve$auc*100, digits=2) #find Auc

#Plot ROC's using ggroc
g2 <- ggroc(list("Log Reg"=ROC_lr_improve))

g2+ggtitle(paste("Log Reg=",AUC_lr_improve,"%",sep="" ))+geom_segment(aes(x = 1, xend = 0, y = 0, yend = 1), color="grey", linetype="dashed")
```

###confusion matrix

```{r confusion matrix-log reg,eval=FALSE}

cutoff=0.2

predictions<- logreg_spy_0%>% predict(combined_test, type = "response") #predict the probabilities

one_or_zero<-ifelse(predictions>cutoff,"X1","X0") #If the the probability is great than the threshold of cutoff then output X1 otherwise X0
p_class<-factor(one_or_zero,levels=levels(combined_test$direction)) #this is a vector of predictions of testing data

con2<-confusionMatrix(p_class,combined_test$direction,positive="X1") #the first input is the class prediction, the second input is the actual outcomes. We also define the positive outcome to be "1" (i.e., default is the outcome we consider "positive"). The output is a confusion matrix.
con2

```

### lasso

```{r lasso regression,eval=FALSE}
#lasso regression method

control <- trainControl (
    method="cv",
    number=10,
    verboseIter=TRUE) #by setting this to true the model will report its progress after each estimation


lambda_seq <- seq(0.004, 0.007, length = 10)
#lasso regression using k-fold cross validation to select the best lambda

lasso <- train(
 direction ~.-date-change,
 data = combined_train,
 method = "glmnet",
  preProc = c("center", "scale"), #This option standardizes the data before running the LASSO regression
  trControl = control,
  tuneGrid = expand.grid(alpha = 1, lambda = lambda_seq) #alpha=1 specifies to run a LASSO regression. If alpha=0 the model would run ridge regression.
  )

# Model coefficients
coef(lasso$finalModel, lasso$bestTune$lambda)
#Best lambda
lasso$bestTune$lambda
# Count of how many coefficients are greater than zero and how many are equal to zero
print("count:")
sum(coef(lasso$finalModel, lasso$bestTune$lambda)!=0)
sum(coef(lasso$finalModel, lasso$bestTune$lambda)==0)

# Make predictions

lasso$xlevels[["date"]] <- union(lasso$xlevels[["date"]], levels(combined_test$date)) #make sure all the levels are recognized

predictions<- lasso%>% predict(combined_test, type = "prob") #predict the probabilities
#summary(predictions)

ROC_lr_improve <- roc(combined_test$direction, predictions$X1) #find roc
AUC_lr_improve<-round(ROC_lr_improve$auc*100, digits=2) #find Auc
print(ROC_lr_improve)

```

### decision tree

```{r decision tree,eval=FALSE}
library(rpart)
library(rpart.plot)
control=rpart.control(cp = 0, maxdepth = 10,minbucket=50,minsplit=2)
LC_treeModel <- rpart(default~., data=train,  method = "class",control =control)
rpart.plot(LC_treeModel)
varImp(LC_treeModel)

defprob_trees<-predict(LC_treeModel, newdata = combined_test,type = "prob")[,2]

ROC_tree <- roc(combined_test$default, defprob_trees)
AUC_tree<-round(ROC_tree$auc*100, digits=2)

```

### grid search 

```{r grid search for decision tree,eval=FALSE}
Grid <- expand.grid(cp = seq(0.0001,0.0002,by=0.00001))

dtree_fit <- train(default~., data=train,
                   method = "rpart",
                   metric="ROC",
                   trControl=train_control,
                   control=rpart.control(minbucket = 25),
                   tuneGrid=Grid) 


# Plot the best tree model found
rpart.plot(dtree_fit$finalModel)
rpart.plot(dtree_fit$finalModel,extra = 1,type=5)

# Print the search results of 'train' function
plot(dtree_fit)
print(dtree_fit)

defprob_trees<-predict(dtree_fit, newdata = combined_test,type = "prob")[,2]

ROC_tree <- roc(combined_test$default, defprob_trees)
AUC_tree<-round(ROC_tree$auc*100, digits=2)
AUC_tree

varImp(dtree_fit)
```

### random forest

```{r random forest,eval=FALSE}
train_control <- trainControl(method="cv", number=2, classProbs=TRUE, 
                        summaryFunction=twoClassSummary,verboseIter = TRUE)
# try various tuning grid parameters
gridRF <- data.frame(
  .mtry = seq(1,5.5,0.5),
  .splitrule = "gini",
  .min.node.size = c(1:5)
)
# Fit random forest: model
rf_RF <- train(
  direction~.-date -change, data=combined_train, 
  method = "ranger",
  metric="ROC",
  trControl = train_control,
  tuneGrid = gridRF,
  importance = 'permutation'
)

# Print model to the console

print(rf_RF)
names(rf_RF$modelInfo)
# Let's check the variable importance
rfImp <- varImp(rf_RF, scale = FALSE)
plot(rfImp)

defprob_RF <- predict(rf_RF,combined_test, type = "prob")[,2]
ROC_forest <- roc(combined_test$direction, defprob_RF)
AUC_forest=round(ROC_forest$auc*100, digits=2)

# show the densities
ggplot( combined_test, aes( defprob_RF, color = as.factor(direction) ) ) +
 geom_density( size = 1 ) +
 ggtitle( "Test Set's Predicted Score" ) +
 xlab("Estimated Probability")

# show the cdf
ggplot(combined_test, aes(defprob_RF, color = as.factor(direction) ) ) +
 stat_ecdf( size = 1 ) +
 ggtitle( "Test Set's Predicted Score" ) +
 xlab("Estimated Probability")
```

### GBM

```{r GBM ,eval=FALSE}

#GBM model with parameter tuning

train_control <- trainControl(method="cv", number=5, classProbs=TRUE, 
                        summaryFunction=twoClassSummary,verboseIter = TRUE)

ctrl <- train_control
grid<-expand.grid(interaction.depth = c(2:3),n.trees = c(200),shrinkage =c(0.03), n.minobsinnode = 10)

gbmFit1 <-  train(
               direction ~ .-change -date, data=combined_train, 
                 method = "gbm", 
                 trControl = ctrl,
                   metric = "ROC" ,
                 preProcess = c("center", "scale"),
                tuneGrid=grid,
                verbose=FALSE
                 )
summary(gbmFit1)
print(gbmFit1)



prob_GBM <-predict(gbmFit1,combined_test, type = "prob")[,2]

ROC_GBM <- suppressMessages(roc(combined_test$direction, prob_GBM))
AUC_GBM=round(auc(ROC_GBM)*100, digits=2)


plot(ROC_GBM, col = "blue",main=paste("GBM AUC=",AUC_GBM,"%",sep = ""))


ggplot(combined_test, aes(prob_GBM, color = as.factor(direction) ) ) +
 stat_ecdf( size = 1 ) +
 ggtitle( "Test Set's Predicted Score" ) +
 xlab("Estimated Probability")

```

### Stacking

```{r stacking, message=FALSE,eval=FALSE}
#Stacking 
library(caretEnsemble)

my_control <- trainControl(
    method="cv",
    number=5,
    savePredictions="final",
    classProbs=TRUE,
    summaryFunction=twoClassSummary,
    verboseIter = TRUE
  )
  
model_list <- caretList(
    direction ~ .-change -date, 
    data=combined_train, 
    trControl=my_control,
    metric = "ROC",
    #methodList=c("gbm"),
    preProcess = c("center", "scale"),
    tuneList=list( ##Change the paramters with the best parameters you found above 
      ranger=caretModelSpec(method="ranger", tuneGrid=data.frame(mtry=1.5,splitrule="gini",min.node.size=2)),
      gbm=caretModelSpec(method="gbm", tuneGrid=data.frame(interaction.depth = 2,n.trees = 200,shrinkage =0.03, n.minobsinnode = 10),verbose=FALSE),
      glmnet = caretModelSpec(method = "glmnet", tuneGrid = data.frame(alpha = 1, lambda = 0.004)))
               )

  
summary(model_list)  
  
modelCor(resamples(model_list))
resamples <- resamples(model_list)
dotplot(resamples, metric = "ROC")
glm_ensemble <- caretStack(
    model_list,
    method="glm",
    metric="ROC",
    trControl=trainControl(
      method="cv",
      number=2,
      savePredictions="final",
      classProbs=TRUE,
      summaryFunction=twoClassSummary
    )
  )

```

### Stacking plot

```{r stacking plot,eval=FALSE}
#Display AUC
#Check the summary of the results
summary(glm_ensemble)    

#Plot the ROC
se_prob<-predict(glm_ensemble,combined_test, type = "prob")#[,2]

ROC_se <- roc(combined_test$direction,se_prob)
# Calculate the area under the curve (AUC)
AUC_se<-round(auc(ROC_se)*100, digits=2)
# Plot the ROC curve
#plot(ROC_se, col = "blue",main=paste("Stack Ensemble AUC=",AUC_se,"%",sep = ""))

g2 <- ggroc(list("RF"=ROC_forest, "Stack" = ROC_se))
g2+ggtitle(paste( "RF=",AUC_forest,"%", " vs Stack=", AUC_se,"%",sep="" ))+geom_segment(aes(x = 1, xend = 0, y = 0, yend = 1), color="grey", linetype="dashed")
```

### clustering

```{r,eval=FALSE}
model_km2 <- kmeans(whisky_tasting_notes, centers = 2,nstart=50)
#Let's check the components of this object.
summary(model_km2)

# add it to the original data frame
whisky_tasting_notes_withClusters<-mutate(whisky_tasting_notes, 
                                   cluster = as.factor(model_km2$cluster))

a<-ggplot(whisky_tasting_notes_withClusters, aes(x = Medicinal, y = Sweetness, color =  as.factor(cluster),size=Smoky)) +
  geom_jitter()+labs(color = "Cluster")

#First let's find the averages of the variables by cluster
center_locations <- whisky_tasting_notes_withClusters%>% group_by(cluster) %>% summarize_at(vars(Body:Floral),mean)

#Next I use gather to collect information together
xa2<- gather(center_locations, key = "variable", value = "value",-cluster,factor_key = TRUE)

#Next I use ggplot to visualize centers
knnCenters<-ggplot(xa2, aes(x = variable, y = value))+  geom_line(aes(color = cluster,group = cluster), linetype = "dashed",size=1)+ geom_point(size=2,shape=4)+geom_hline(yintercept=0)+ggtitle("K-means centers k=2")+labs(fill = "Cluster")+theme(text = element_text(size=10),axis.text.x = element_text(angle=45, hjust=1),legend.title=element_text(size=5),legend.text = element_text(size=5))
knnCenters


##elbow chart

library(purrr) #a package for writing succinctfor loops

# Use map_dbl to run K-Means models with varying value of k 
tot_withinss <- map_dbl(1:10,  function(k){
  model <- kmeans(x = whisky_tasting_notes, centers = k,iter.max = 100, nstart = 10)
  model$tot.withinss
})

# Generate a data frame containing both k and tot_withinss
elbow_df <- data.frame(
  k = 1:10 ,
  tot_withinss = tot_withinss
)

# Plot the elbow plot
ggplot(elbow_df, aes(x = k, y = tot_withinss)) +
  geom_line() +
  scale_x_continuous(breaks = 1:10)
```


### PCA

```{r,eval=FALSE}
library(factoextra)
fviz_cluster(model_km2, whisky_tasting_notes,
             palette = "Set2", ggtheme = theme_minimal())

model_km2 <- eclust(whisky_tasting_notes, "kmeans", k = 2,nstart = 50, graph = FALSE)
model_km2$size

p1 <- fviz_cluster(model_km2, geom = "point", data = whisky_tasting_notes) + ggtitle("k = 2")

#k2_pam <-eclust(whisky_tasting_notes, "pam", k = k, graph = FALSE) #for PAM

library(gridExtra)
grid.arrange(p1, p2,p3,p4, nrow = 2)

```

### KNN

```{r,eval=FALSE}
fit_KNN <- train(watched_in_M2~.-user_id, data=combined_train,
                 preProcess = c("center", "scale"), 
                 method="knn", 
                 metric="ROC", 
                 trControl=control,
                 tuneGrid = knnGrid)

knn_class<-predict(fit_KNN, newdata = combined_test, cutoff = .5 )

###Look at the confusion matrix using 'confusionMatrix' from 'caret' library
confusionMatrix(data = knn_class,reference = combined_test$watched_in_M2) 

suppressMessages(library(pROC))

# I use 'predict' function to estimate probabilities. The result has two columns; first the probabilty of being class 0 and second # for being 1. So I take the second column 
knn_probabilites_train<-predict(fit_KNN,type = "prob")[,2]
knn_probabilites<-predict(fit_KNN, newdata = combined_test,type = "prob")[,2]

# Let's find the ROC values using 'roc' function from pROC. 
ROC_knn <- roc(combined_test$watched_in_M2, knn_probabilites)
# Let's find AUC using the 'auc' function and round it for ease of notation. 
AUC_knn<-round(auc(ROC_knn)*100, digits=2)

```


### Word Cloud

```{r wordcloud,eval=FALSE}
txt<- Corpus(VectorSource(discrepancy$message))
docs <- txt
class(docs)
inspect(docs[1:5])


#stopwords("english")
docs <- tm_map(docs, tolower)
docs <- tm_map(docs, removeNumbers)
docs <- tm_map(docs, removeWords, stopwords("english"))
docs <- tm_map(docs, removePunctuation)
docs <- tm_map(docs, stripWhitespace)
words <- c("can", "will", "one", "two", "three", "four","usa","america","aaas")
docs <- tm_map(docs, removeWords, words)
inspect(docs[[1]])

#' Do not stem straight away, as wordcloud won't be as interpretable
#docs <- tm_map(docs, stemDocument, language = "english")  

DTM <- DocumentTermMatrix(docs) 
freq <- colSums(as.matrix(DTM)) 
ord <- sort(freq, decreasing = TRUE) # descending order
ord[1:5]
# now  get just call ltgt 
# 323  316  301  281  276 

wordFreqTable <- table(freq)
tail(wordFreqTable) 


# Find which words occurred at least 125 times
top_10 <- findFreqTerms(DTM, 210)
top_10
# "got"  "like" "just" "know" "now"  "ltgt" "get"  "call" "good" "come"

#' bar chart of n most freq words
df <- data.frame(word = names(ord), freq = ord)
head(df)
wf <- df[1:10, ]
barplot(wf$freq, names = wf$word, 
        main = "Word Frequency", 
        cex.lab = 2, 
        cex.axis = 2.0,
        cex.main = 2.5,
        ylim =c(0, 250), xlab = "Words", ylab = "Counts")


wordcloud2(data = df[1:200,], color = "random-light", backgroundColor = "grey") 
```

